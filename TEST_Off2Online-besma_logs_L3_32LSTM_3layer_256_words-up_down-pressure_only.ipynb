{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15150308560345827756\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time \n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.layers.core import Dense\n",
    "import cv2\n",
    "#print (cv2.__version__)\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Architecture(Input, outputs, is_training=True):\n",
    "    #Input,_,_=load_patch_data(Data_Train_Patch,heught=224,win_size=224,win_step=112,return_all=True)\n",
    "    #Input.shape\n",
    "    with tf.variable_scope(\"Network\"):\n",
    "        \n",
    "        with tf.variable_scope(\"CNN\"):\n",
    "            #x_image = tf.reshape(image_batch, [-1, 64, 64, 3]) c moi besma, après on utilise x_image as son input\n",
    "            conv1=tf.layers.conv2d(Input,64,3,activation=tf.nn.relu,name='conv1',padding='same')\n",
    "            pool1=tf.layers.max_pooling2d(conv1,2,[2,2],name='pool1')\n",
    "            #print('shape conv1',conv1.shape)\n",
    "            #print('shape pool1 [2,2]',pool1.shape)\n",
    "            #print('************************')\n",
    "            \n",
    "            conv2=tf.layers.conv2d(pool1,128,3,activation=tf.nn.relu,name='conv2',padding='same')\n",
    "            pool2=tf.layers.max_pooling2d(conv2,2,[2,1],name='pool2')\n",
    "           # print('shape conv2',conv2.shape)\n",
    "           # print('shape pool2 [2,1]',pool1.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv3=tf.layers.conv2d(pool2,256,3,activation=None,name='conv3',padding='same')\n",
    "            b_norm3=tf.nn.relu(tf.layers.batch_normalization(conv3,training=is_training,name='batch-norm1'),name='b_norm3')\n",
    "           # print('shape conv3',conv3.shape)\n",
    "            #print('shape b_norm3',b_norm3.shape)\n",
    "            #print('************************')\n",
    "            \n",
    "            conv4=tf.layers.conv2d(b_norm3,256,3,activation=tf.nn.relu,name='conv4',padding='same')\n",
    "            pool4=tf.layers.max_pooling2d(conv4,2,[2,1],name='pool4',padding='same')\n",
    "           # print('shape conv4',conv4.shape)\n",
    "           # print('shape pool4 [2,1]',pool4.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv5=tf.layers.conv2d(pool4,256,3,activation=tf.nn.relu,name='conv5',padding='same')\n",
    "            pool5=tf.layers.max_pooling2d(conv5,2,[2,1],name='pool5',padding='same')\n",
    "           # print('shape conv5',conv5.shape)\n",
    "           # print('shape pool5 [2,1]',pool5.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv6=tf.layers.conv2d(pool5,256,3,activation=None,name='conv6',padding='same')\n",
    "            b_norm6=tf.nn.relu(tf.layers.batch_normalization(conv6,training=is_training,name='batch-norm2'),name='b_norm6')\n",
    "           # print('shape conv6',conv6.shape)  \n",
    "           # print('shape b_norm6',b_norm6.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv7=tf.layers.conv2d(b_norm6,256,3,activation=tf.nn.relu,name='conv7',padding='same')\n",
    "            pool7=tf.layers.max_pooling2d(conv7,2,[2,1],name='pool7',padding='same')\n",
    "           # print('shape conv7',conv7.shape)\n",
    "           # print('shape pool7[2,1]',pool7.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv8=tf.layers.conv2d(pool7,256,2,activation=None,name='conv8')\n",
    "            b_norm8=tf.nn.relu(tf.layers.batch_normalization(conv8,training=is_training,name='batch-norm3'),name='b_norm8')\n",
    "           # print('shape conv8',conv8.shape)\n",
    "           # print('shape b_norm8',b_norm8.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            shape=b_norm8.get_shape().as_list()\n",
    "            #print('shape of shape norm8',b_norm8.shape)#shape norm8 (32, 1, 30, 256)\n",
    "            transposed=tf.transpose(b_norm8, perm=[0,2,1,3],name='transposed')\n",
    "            conv_reshaped=tf.reshape(transposed, [shape[0], -1, shape[1]*shape[3]], name='reshaped')\n",
    "            #print('shape conv_reshaped',conv_reshaped.shape)#shape conv_reshaped (32, 30, 256)\n",
    "            \n",
    "            \n",
    "            \n",
    "        num_units=256\n",
    "        num_layers=3\n",
    "        \n",
    "\n",
    "       \n",
    "        cell_stack_encoder=[]\n",
    "        cell_stack_decoder=[]    \n",
    "        \n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            for i in range(num_layers):\n",
    "                cell_stack_encoder.append(tf.contrib.rnn.LSTMCell(num_units))\n",
    "            encoder_cell=tf.contrib.rnn.MultiRNNCell(cell_stack_encoder, state_is_tuple=True)\n",
    "            _, encoder_state=tf.nn.dynamic_rnn(encoder_cell, conv_reshaped, dtype=tf.float32) \n",
    "            print('encoder_state_shape',len(encoder_state))\n",
    "              \n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            for i in range(num_layers):\n",
    "                cell_stack_decoder.append(tf.contrib.rnn.LSTMCell(num_units))\n",
    "            decoder_cell=tf.contrib.rnn.MultiRNNCell(cell_stack_decoder, state_is_tuple=True)\n",
    "            dec_outputs, _=tf.nn.dynamic_rnn(decoder_cell, outputs, initial_state= encoder_state, dtype=tf.float32)\n",
    "            print('dec_outputs', dec_outputs.get_shape())\n",
    "            print('outputs=dec_inputs', outputs.get_shape())\n",
    "            \n",
    "            #logits=tf.layers.dense(dec_outputs,2, name='logits')\n",
    "            logits=tf.layers.dense(dec_outputs,1, name='logits')\n",
    "            \n",
    "            return logits\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Architecture(Input, outputs, is_training=True):\n",
    "    #Input,_,_=load_patch_data(Data_Train_Patch,heught=224,win_size=224,win_step=112,return_all=True)\n",
    "    #Input.shape\n",
    "    with tf.variable_scope(\"Network\"):\n",
    "        \n",
    "        with tf.variable_scope(\"CNN\"):\n",
    "            #x_image = tf.reshape(image_batch, [-1, 64, 64, 3]) c moi besma, après on utilise x_image as son input\n",
    "            conv1=tf.layers.conv2d(Input,64,3,activation=tf.nn.relu,name='conv1',padding='same')\n",
    "            pool1=tf.layers.max_pooling2d(conv1,2,[2,2],name='pool1')\n",
    "            #print('shape conv1',conv1.shape)\n",
    "            #print('shape pool1 [2,2]',pool1.shape)\n",
    "            #print('************************')\n",
    "            \n",
    "            conv2=tf.layers.conv2d(pool1,128,3,activation=tf.nn.relu,name='conv2',padding='same')\n",
    "            pool2=tf.layers.max_pooling2d(conv2,2,[2,1],name='pool2')\n",
    "           # print('shape conv2',conv2.shape)\n",
    "           # print('shape pool2 [2,1]',pool1.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv3=tf.layers.conv2d(pool2,256,3,activation=None,name='conv3',padding='same')\n",
    "            b_norm3=tf.nn.relu(tf.layers.batch_normalization(conv3,training=is_training,name='batch-norm1'),name='b_norm3')\n",
    "           # print('shape conv3',conv3.shape)\n",
    "            #print('shape b_norm3',b_norm3.shape)\n",
    "            #print('************************')\n",
    "            \n",
    "            conv4=tf.layers.conv2d(b_norm3,256,3,activation=tf.nn.relu,name='conv4',padding='same')\n",
    "            pool4=tf.layers.max_pooling2d(conv4,2,[2,1],name='pool4',padding='same')\n",
    "           # print('shape conv4',conv4.shape)\n",
    "           # print('shape pool4 [2,1]',pool4.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv5=tf.layers.conv2d(pool4,256,3,activation=tf.nn.relu,name='conv5',padding='same')\n",
    "            pool5=tf.layers.max_pooling2d(conv5,2,[2,1],name='pool5',padding='same')\n",
    "           # print('shape conv5',conv5.shape)\n",
    "           # print('shape pool5 [2,1]',pool5.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv6=tf.layers.conv2d(pool5,256,3,activation=None,name='conv6',padding='same')\n",
    "            b_norm6=tf.nn.relu(tf.layers.batch_normalization(conv6,training=is_training,name='batch-norm2'),name='b_norm6')\n",
    "           # print('shape conv6',conv6.shape)  \n",
    "           # print('shape b_norm6',b_norm6.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv7=tf.layers.conv2d(b_norm6,256,3,activation=tf.nn.relu,name='conv7',padding='same')\n",
    "            pool7=tf.layers.max_pooling2d(conv7,2,[2,1],name='pool7',padding='same')\n",
    "           # print('shape conv7',conv7.shape)\n",
    "           # print('shape pool7[2,1]',pool7.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            conv8=tf.layers.conv2d(pool7,256,2,activation=None,name='conv8')\n",
    "            b_norm8=tf.nn.relu(tf.layers.batch_normalization(conv8,training=is_training,name='batch-norm3'),name='b_norm8')\n",
    "           # print('shape conv8',conv8.shape)\n",
    "           # print('shape b_norm8',b_norm8.shape)\n",
    "           # print('************************')\n",
    "            \n",
    "            shape=b_norm8.get_shape().as_list()\n",
    "            #print('shape of shape norm8',b_norm8.shape)#shape norm8 (32, 1, 30, 256)\n",
    "            transposed=tf.transpose(b_norm8, perm=[0,2,1,3],name='transposed')\n",
    "            conv_reshaped=tf.reshape(transposed, [shape[0], -1, shape[1]*shape[3]], name='reshaped')\n",
    "            #print('shape conv_reshaped',conv_reshaped.shape)#shape conv_reshaped (32, 30, 256)\n",
    "            \n",
    "            \n",
    "            \n",
    "        num_units=256\n",
    "        num_layers=3\n",
    "        \n",
    "\n",
    "       \n",
    "        cell_stack_encoder=[]\n",
    "        cell_stack_decoder=[]    \n",
    "        \n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            for i in range(num_layers):\n",
    "                cell_stack_encoder.append(tf.contrib.rnn.GRUCell(num_units))\n",
    "           # encoder_cell=tf.contrib.rnn.MultiRNNCell(cell_stack_encoder, state_is_tuple=True)\n",
    "            encoder_cell=tf.contrib.rnn.MultiRNNCell(cell_stack_encoder)\n",
    "               # cell_stack_encoder.append(tf.contrib.rnn.LSTMCell(num_units))\n",
    "           # encoder_cell=tf.contrib.rnn.MultiRNNCell(cell_stack_encoder, state_is_tuple=True)\n",
    "            _, encoder_state=tf.nn.dynamic_rnn(encoder_cell, conv_reshaped, dtype=tf.float32) \n",
    "           \n",
    "            #encoder_state = tuple(tf.concat([state_fw, state_bw], -1) for state_fw, state_bw in zip(encoder_state[0], encoder_state[1]))\n",
    "           # print('encoder_state_shape',len(encoder_state))\n",
    "             \n",
    "            print('encoder_state', _.get_shape())\n",
    "              \n",
    "        with tf.variable_scope(\"Decoder\"):\n",
    "            for i in range(num_layers):\n",
    "                cell_stack_decoder.append(tf.contrib.rnn.GRUCell(num_units))\n",
    "            decoder_cell=tf.contrib.rnn.MultiRNNCell(cell_stack_decoder)\n",
    "            \n",
    "            dec_outputs,_=tf.nn.dynamic_rnn(decoder_cell,outputs, initial_state=encoder_state, dtype=tf.float32)\n",
    "            \n",
    "            #logits=tf.layers.dense(dec_outputs,2, name='logits')\n",
    "            logits=tf.layers.dense(dec_outputs,1, name='logits')\n",
    "            \n",
    "            return logits\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_DataTest  [[[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]\n",
      "\n",
      "\n",
      " [[[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]\n",
      "\n",
      "  [[0]\n",
      "   [0]\n",
      "   [0]\n",
      "   ...\n",
      "   [0]\n",
      "   [0]\n",
      "   [0]]]] 61.tif\n",
      "encoder_state (2, 73, 256)\n",
      "INFO:tensorflow:Restoring parameters from /home/razen/tf-notebooks/logspenup_GRU_words3/logspenup_GRU_words3.ckpt-1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [64] rhs shape= [50]\n\t [[Node: save/Assign_12 = Assign[T=DT_FLOAT, _class=[\"loc:@Network/CNN/conv1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Network/CNN/conv1/bias, save/RestoreV2_12)]]\n\nCaused by op u'save/Assign_12', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/razen/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/razen/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/razen/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/razen/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/razen/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/razen/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/razen/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-96c82b2d5a39>\", line 81, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1239, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1248, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1284, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 765, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 440, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 160, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 59, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [64] rhs shape= [50]\n\t [[Node: save/Assign_12 = Assign[T=DT_FLOAT, _class=[\"loc:@Network/CNN/conv1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Network/CNN/conv1/bias, save/RestoreV2_12)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-96c82b2d5a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m#meta = tf.train.import_meta_graph('logs_L2model_logs_L2.ckpt-1.meta')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_train_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;31m#meta.restore(sess, tf.train.latest_checkpoint('./'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m#saver.restore(sess, model_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1684\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1686\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1687\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [64] rhs shape= [50]\n\t [[Node: save/Assign_12 = Assign[T=DT_FLOAT, _class=[\"loc:@Network/CNN/conv1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Network/CNN/conv1/bias, save/RestoreV2_12)]]\n\nCaused by op u'save/Assign_12', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/razen/.local/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/home/razen/.local/lib/python2.7/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/razen/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/home/razen/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/razen/.local/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/razen/.local/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/razen/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/razen/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/razen/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-96c82b2d5a39>\", line 81, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1239, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1248, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1284, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 765, in _build_internal\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 440, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 160, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py\", line 276, in assign\n    validate_shape=validate_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py\", line 59, in assign\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [64] rhs shape= [50]\n\t [[Node: save/Assign_12 = Assign[T=DT_FLOAT, _class=[\"loc:@Network/CNN/conv1/bias\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Network/CNN/conv1/bias, save/RestoreV2_12)]]\n"
     ]
    }
   ],
   "source": [
    "def load_Images(Data_Train_Path):\n",
    "    #main_path=[f for f in np.sort(os.listdir(Data_Train_Path)) if f.endswith('redraw')]\n",
    "    main_path=[f for f in np.sort(os.listdir(Data_Train_Path)) if f.endswith('redraw')]\n",
    "    height =28\n",
    "    imgs=[]\n",
    "    for s in main_path:\n",
    "        sub=Data_Train_Path+s\n",
    "        sub_path= [f for f in np.sort(os.listdir(sub))if f.endswith('.tif')]\n",
    "        for t in sub_path:\n",
    "            im_path=sub + '/' +t\n",
    "            img=cv2.imread(im_path,cv2.IMREAD_UNCHANGED)\n",
    "            imgs.append(img)\n",
    "            #plt.imshow(img)\n",
    "            #plt.show()\n",
    "           # print('name image',t)\n",
    "    #return np.expand_dims(imgs, -1,f)\n",
    "    return (np.expand_dims(imgs, -1),t)\n",
    "\n",
    "\n",
    "##*****************************************************************************************************\n",
    "#saver.restore(sess, tf.train.latest_checkpoint('./') )  \n",
    "#Save_folder = '/home/razen/thèse_2018/second_contrib/code/Besma_code_Recovery/recover_IRONOFF_caracters/systeme_2018/data_train_test/New_test_40_echantillons/recovered_2018/aa/'\n",
    "Save_folder ='/home/razen/thèse_2018/second_contrib/code/Besma_code_Recovery/recover_IRONOFF_Words/ironoffwords/test_word/recov/'\n",
    "#Save_folder ='/home/razen/thèse_2018/second_contrib/code/Besma_code_Recovery/recover_IRONOFF_caracters/digit_2018/data_test/00/'\n",
    "#model_train_path = \"/home/razen/tf-notebooks/logs_L3_28_digits/logs_L3_28_digits.ckpt-1\"\n",
    "model_train_path = \"/home/razen/tf-notebooks/logspenup_GRU_words3/logspenup_GRU_words3.ckpt-1\"\n",
    "\n",
    "#model_train_path = \"/home/razen/tf-notebooks/logspenup_words/logspenup_words.ckpt-1\"\n",
    "#model_train_path = \"/home/razen/tf-notebooks/model_logs_4_alv/logs_L4model_logs_L4_alv.ckpt-1\"\n",
    "number = 0\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "tf.reset_default_graph()\n",
    "global sess\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session (config=config)\n",
    "graph = tf.get_default_graph()\n",
    "import os\n",
    "#my_DataTest = load_Images('/home/razen/tf-notebooks/New_ironoff_test_10_echantillons/')\n",
    "my_DataTest,f = load_Images('/home/razen/thèse_2018/second_contrib/code/Besma_code_Recovery/recover_IRONOFF_Words/ironoffwords/test_word/')\n",
    "#my_DataTest,f = load_Images('/home/razen/thèse_2018/second_contrib/code/Besma_code_Recovery/recover_IRONOFF_caracters/systeme_2018/data_train_test/New_test_40_echantillons/redraw/')\n",
    "#_,f=load_Images('/home/razen/tf-notebooks/IRONOFF_Characters_test_results_40/test_40_char_ironoff/usr_0_redraw_classed/redraw/')\n",
    "print('my_DataTest ',my_DataTest,f)\n",
    "\n",
    "\n",
    "#my_LabelTest_Targets, my_LabelTest_Outputs = load_Coordinates ('/home/razen/tf-notebooks/IRONOFF_Characters_test_results/test_folder/usr_0_online_classed/')\n",
    "[Tot_Dat, row, col, depth] = np.shape(my_DataTest)\n",
    "#rand_idx = np.random.choice(Tot_Dat, 260, replace=False)\n",
    "#rand_idx = np.random.choice(Tot_Dat, 40, replace=False)\n",
    "#my_Data_Test = my_DataTest[rand_idx,:,:,:]\n",
    "#for i_im in range(40):\n",
    "#    my_Data_Test = my_DataTest[i_im,:,:,:]\n",
    "#    print('my_Data_Test index',my_Data_Test.index)\n",
    "\n",
    "#my_LabelTest_Outputs = my_LabelTest_Outputs[rand_idx,:,:]\n",
    "#my_LabelTest_Targets = my_LabelTest_Targets[rand_idx,:,:]\n",
    "\n",
    "\n",
    "BatchSize_Test = 2\n",
    "\n",
    "Inputs = tf.placeholder(tf.float32, shape = [BatchSize_Test, 50, 150, 1],name = 'Inputs')\n",
    "outputs = tf.placeholder(tf.float32, shape = [BatchSize_Test, None, 1],name = 'outputs')\n",
    "#targets = tf.placeholder(tf.float32, shape = [BatchSize_Test, 50, 2],name = 'targets')\n",
    "\n",
    "    \n",
    "logits = Architecture(Inputs, outputs)\n",
    "#loss = Loss_function(logits, targets)\n",
    "\n",
    "#init = tf.global_variables_initializer()\n",
    "#sess.run(init)\n",
    "#saver.restore(sess, 'logs_L2model_logs_L2' )  \n",
    "#############################################train accuracy#####################################################\n",
    "#correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#accuracy = np.mean(logits.argmax(axis=-1) == targets[:,1:])\n",
    "################################################################################################################\n",
    "#tf.reset_default_graph()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    #meta = tf.train.import_meta_graph('logs_L2model_logs_L2.ckpt-1.meta')\n",
    "    saver.restore(sess, model_train_path)\n",
    "    #meta.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    #saver.restore(sess, model_path)\n",
    "    #print(\"Model restored from file: %s\" % save_path)\n",
    "    \n",
    "    #saver.restore(sess, tf.train.latest_checkpoint('./') )  \n",
    "    #meta.restore(sess, tf.train.latest_checkpoint('./')\n",
    "                 \n",
    "    \n",
    "     #print(sess.run(Y, feed_dict={x_placeholder: X))\n",
    "\n",
    "\n",
    "#for k in range(3):\n",
    "    start = time.time()\n",
    "    Test_loss = []\n",
    "\n",
    "    \n",
    "    for b in range(Tot_Dat//BatchSize_Test):\n",
    "        batch_Test_input = my_DataTest[b*BatchSize_Test:(b + 1)*BatchSize_Test,:,:,:]\n",
    "        print('b',b)\n",
    "        print('debut b*BatchSize_Test',b*BatchSize_Test)\n",
    "        print('fin batch_Test_input index',(b + 1)*BatchSize_Test)\n",
    "        \n",
    "        #batch_Test_targets = my_LabelTest_Targets[b*BatchSize_Test:(b + 1)*BatchSize_Test,:,:]\n",
    "        batch_Test_outputs = np.zeros(shape = [BatchSize_Test,1,1])\n",
    "                #batch_Test_input = Data_Test[6,:,:,:]\n",
    "                #batch_Test_targets = LabelTest_Targets[6,:,:]\n",
    "                #batch_Test_outputs = np.zeros(shape = [BatchSize_Test,1,2])\n",
    "                    \n",
    "        for ij in range(100):\n",
    "                #test_feed_dict = {batch_Test_input, batch_Test_outputs}\n",
    "            test_feed_dict = {Inputs:batch_Test_input, outputs:batch_Test_outputs}\n",
    "            logits_Out = sess.run([logits], feed_dict = test_feed_dict)\n",
    "            \n",
    "            logits_Out = logits_Out[0]\n",
    "                \n",
    "            prediction = np.expand_dims(logits_Out[:,-1,:],axis = 1)\n",
    "            batch_Test_outputs = np.hstack([batch_Test_outputs, prediction])\n",
    "            #acc= sess.run([accuracy], feed_dict={logits: logits_Out,targets:batch_Test_targets})            \n",
    "                #np.savetxt('logits_Out[0]', logits_Out[0])\n",
    "                #print('logits_Out:',np.array(logits_Out))\n",
    "                #print('logits_Out[0]:',np.array(logits_Out[0]))\n",
    "                #print('logits_Out[1]:',np.array(logits_Out[1]))\n",
    "                #print('logits_Out_shape:',np.array(logits_Out.shape))\n",
    "        #loss_ = Loss_function_Test(logits_Out, batch_Test_targets)\n",
    "        #loss_ = tf.losses.mean_squared_error(logits_Out, batch_Test_targets)\n",
    "        #loss_=tf.metrics.root_mean_squared_error(logits_Out, batch_Test_targets)\n",
    "         #loss_=tf.sqrt(tf.losses.mean_squared_error(logits_Out, batch_Test_targets))\n",
    "        #file = open(\"./output/\" + \"logits_pred33.txt\",\"a\") \n",
    "        \n",
    "        #file.write(str(logits_Out[0]) + \"\\t\" +str(batch_Test_targets)+\"\\t\" \"\\n\")\n",
    "\n",
    "       # file.close()\n",
    "        #Test_loss.append(loss_)\n",
    "        \n",
    "\n",
    "        ############################################\n",
    "        for num in range(2):\n",
    "            #directory = Save_folder + str(number) + '/'\n",
    "            directory = Save_folder \n",
    "            try:\n",
    "                os.stat(directory)\n",
    "            except:\n",
    "                os.mkdir(directory) \n",
    "           # img = 255 - batch_Test_input[num, :,:,0]\n",
    "            #imgpath = directory + str(number) + '.jpg'\n",
    "            #cv2.imwrite(imgpath, img)\n",
    "            #np.savetxt(  directory + 'Grountruth_' + str(num)+str(number) + '.txt', batch_Test_targets[num,:,:])\n",
    "            logout_float=logits_Out[num,:,:]\n",
    "            logout_int=logout_float.astype(int)\n",
    "            \n",
    "            np.savetxt(  directory + 'Predict_flaot' + str(num)+str(number) + '.txt', logout_float)\n",
    "            np.savetxt(  directory + 'Predict_txt' + str(num)+str(number) + '.txt', logout_int)\n",
    "            \n",
    "            number = number + 1\n",
    "        #if jk%3 == 0:\n",
    "            #file.close()\n",
    "            #reDraw(batch_Test_targets.astype(int), logits_Out.astype(int),2, ' Testing')\n",
    "        ##########################################\n",
    "    \n",
    "    #print('### TESTING TESTING TESING ###')\n",
    "    #print( 'Test Loss is: => ' + str(Test_loss) + '\\n')\n",
    "    #print('### TESTING TESTING TESING ###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
