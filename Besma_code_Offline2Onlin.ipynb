{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"colab":{"name":"Besma_code_Offline2Onlin.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CcdeJXpTaJ3V"},"source":["%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n","%env CUDA_VISIBLE_DEVICES=1\n","from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85bBOh9taomx"},"source":["import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() \n","tf.reset_default_graph()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTHBbRGXbQXx"},"source":["from tensorflow.python.compiler import tensorrt as trt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-04IE-saJ3c"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","\n","import time\n","\n","import scipy.io\n","import numpy as np\n","import tensorflow as tf\n","import h5py\n","import random\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","import itertools\n","from tensorflow.python.layers.core import Dense\n","#from tensorflow.contrib import layers\n","from tensorflow.python.layers.core import Dense\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hNTuIafaJ3e"},"source":["def load_Images(Data_Train_Path):\n","    main_path=[f for f in np.sort(os.listdir(Data_Train_Path)) if f.endswith('redraww')]\n","    #height =547\n","    imgs=[]\n","    for s in main_path:\n","        sub=Data_Train_Path+s\n","        sub_path= [f for f in np.sort(os.listdir(sub))if f.endswith('.tif')]\n","        for t in sub_path:\n","            im_path=sub + '/' +t\n","            img=cv2.imread(im_path,cv2.IMREAD_UNCHANGED)\n","            img=cv2.resize(img,(64,64))\n","            imgs.append(img)\n","            #plt.imshow(img)\n","            #plt.show()\n","            #print('expend image ',np.expand_dims(imgs, -1))\n","            \n","    return np.expand_dims(imgs, -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ris6FoyJaJ3e"},"source":["#Input \n","def load_Coordinates(Data_Train_Path):\n","    Data_Label = []\n","    DATA_Label_K=[]\n","    main_path = [f for f in np.sort(os.listdir(Data_Train_Path)) if f.endswith('onlinee')]\n","    \n","    for s in main_path:\n","        sub = Data_Train_Path + s        \n","        sub_path = [f for f in np.sort(os.listdir(sub)) if f.endswith('.txt')]\n","        for t in sub_path:\n","            txt_path = sub + '/' +t\n","            file_B = open(txt_path, 'r')\n","            data = file_B.read()\n","            x = data.split('\\n')#\n","            \n","            L=len(x)-1\n","           \n","            online_data = np.ones([50,2],np.int32)\n","            online_data_K = np.ones([51,2],np.int32)\n","          \n","            y=0\n","          \n","            while y <= 50  :               \n","                z = x[y].split(',') \n","                if z == ['']:\n","                    break\n","                \n","               \n","                online_data[y] = [int(z[0]),int(z[1])]\n","                online_data_K[y+1] = [int(z[0]),int(z[1])]\n","                y+=1\n","            online_data_K = online_data_K[:50,:]\n","            file_B.close()\n","            Data_Label.append(online_data)\n","            DATA_Label_K.append(online_data_K) \n","          \n","           \n","    return np.array(Data_Label), np.array(DATA_Label_K)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B9pLLXlaJ3e"},"source":["def Architecture(Input, outputs, is_training = True):\n","             \n","    with tf.variable_scope(\"Network\"):\n","        \n","        with tf.variable_scope(\"CNN\"):\n","            conv1 = tf.layers.conv2d(Input,64,3,activation=tf.nn.relu,name = 'conv1', padding = 'same')\n","            pool1 = tf.layers.max_pooling2d(conv1,2,[2,2],name = 'pool1')\n","\n","            conv2 = tf.layers.conv2d(pool1,128,3,activation=tf.nn.relu,name = 'conv2', padding = 'same')\n","            pool2 = tf.layers.max_pooling2d(conv2, 2,[2,1],name = 'pool2',  padding = 'same')\n","\n","\n","            conv3 = tf.layers.conv2d(pool2,256,3,activation=None,name = 'conv3', padding = 'same')\n","            b_norm3 = tf.nn.relu(tf.layers.batch_normalization(conv3, training = is_training, name='batch-norm1'), name = 'relu3')\n","\n","\n","            conv4 = tf.layers.conv2d(b_norm3,256,3,activation=tf.nn.relu,name = 'conv4', padding = 'same')\n","            pool4 = tf.layers.max_pooling2d(conv4, 2,[2,1],name = 'pool4',  padding = 'same')\n","\n","            conv5 = tf.layers.conv2d(pool4,256,3,activation=tf.nn.relu,name = 'conv5', padding = 'same')\n","            pool5 = tf.layers.max_pooling2d(conv5, 2,[2,1],name = 'pool5',  padding = 'same')\n","\n","            conv6 = tf.layers.conv2d(pool5,256,3,activation=None,name = 'conv6', padding = 'same')\n","            b_norm6 = tf.nn.relu(tf.layers.batch_normalization(conv6, training = is_training, name='batch-norm2'), name = 'relu6')\n","\n","            conv7 = tf.layers.conv2d(b_norm6,256,3,activation=tf.nn.relu,name = 'conv7', padding = 'same')\n","            pool7 = tf.layers.max_pooling2d(conv7, 2,[2,1],name = 'pool7',  padding = 'same')\n","\n","            conv8 = tf.layers.conv2d(pool7,256,2,activation=None, name = 'conv8')\n","            print('conv8',conv8.shape)\n","            b_norm8 = tf.nn.relu(tf.layers.batch_normalization(conv8, training = is_training, name='batch-norm3'), name = 'relu8')\n","            print('b_norm8',b_norm8.shape)\n","\n","            shape = b_norm8.get_shape().as_list()\n","            transposed = tf.transpose(b_norm8, perm=[0, 2, 1, 3], name='transposed') \n","            conv_reshaped = tf.reshape(transposed, [ shape[0], -1, shape[1] * shape[3] ], name='reshaped') \n","            print('conv_reshaped',conv_reshaped.shape)\n","        num_units = 256\n","        num_layers = 3\n","        \n","        cell_stack_encoder = []\n","        cell_stack_decoder = []\n","\n","        with tf.variable_scope(\"Encoder\"):\n","            for i in range(num_layers):\n","                cell_stack_encoder.append(tf.compat.v1.nn.rnn_cell.BasicRNNCell(num_units))\n","            encoder_cell=tf.compat.v1.nn.rnn_cell.MultiRNNCell(cell_stack_encoder, state_is_tuple=True)\n","            _, encoder_state=tf.compat.v1.nn.dynamic_rnn(encoder_cell, conv_reshaped, dtype=tf.float32) \n","\n","        with tf.variable_scope(\"Decoder\"):\n","            for i in range(num_layers):\n","                cell_stack_decoder.append(tf.compat.v1.nn.rnn_cell.BasicRNNCell(num_units))\n","            decoder_cell=tf.compat.v1.nn.rnn_cell.MultiRNNCell(cell_stack_decoder)\n","            \n","            dec_outputs,_=tf.compat.v1.nn.dynamic_rnn(decoder_cell,outputs, initial_state=encoder_state, dtype=tf.float32) \n","            \n","            logits = tf.layers.dense(dec_outputs, 2, name  = 'logits')  \n","        \n","            return logits    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmlwQL1AaJ3f"},"source":["def Loss_function(Dec_Outputs, Targets):\n","    loss = tf.reduce_mean(tf.abs(Dec_Outputs - Targets), name = 'loss')\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jaiwV42Udbk7"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","eps=1e-8\n","from datetime import datetime\n","from tensorflow.python.framework import ops\n","ops.reset_default_graph()\n","global sess\n","config = tf.ConfigProto()\n","config.gpu_options.allow_growth = True\n","sess = tf.Session (config=config)\n","graph = tf.get_default_graph()\n","import os\n","\n","BatchSize = 32                                           #\n","learning_rate = 0.0001 \n","num_epochs = 100\n","\n","model_path = \"gdrive/My Drive/data/lstm-not-attention6_layer.ckpt-1\"\n","\n","\n","LabelTrain_Targets, LabelTrain_Outputs = load_Coordinates ('gdrive/My Drive/data/')                              #\n","DataTrain = load_Images('gdrive/My Drive/data/')                                                                 #\n","print('LabelTrain_Targets shape',LabelTrain_Targets.shape)\n","print('LabelTrain_Outputs shape',LabelTrain_Outputs.shape)\n","  \n","print('DataTrain shape',DataTrain.shape)\n","Data_Train=DataTrain                                                                                      #\n","LabelTrain_Outputs=LabelTrain_Outputs    \n","print('LabelTrain_Outputs dtype',LabelTrain_Outputs.dtype)\n","LabelTrain_Targets=LabelTrain_Targets                                                                     #\n","##################################################Fin Import Data#########################################\n","     \n","###############################################Create tensors of variables################################    \n","Inputs = tf.placeholder(tf.float32, shape = [BatchSize, 64, 64, 1],name = 'Inputs')\n","outputs = tf.placeholder(tf.float32, shape = [BatchSize, None, 2],name = 'outputs')\n","seq_length = tf.placeholder(tf.int32, [None])\n","targets = tf.placeholder(tf.float32, shape = [BatchSize, None, 2],name = 'targets')\n","\n","############################################### Fin Create tensors of variables################################  \n","\n","logits = Architecture(Inputs, outputs)\n","print('targets shape',targets.shape)\n","\n","\n","#############################################train accuracy#####################################################\n","correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(targets, 1))\n","accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","################################################################################################################\n","loss = Loss_function2(logits, targets)\n","\n","\n","  \n","training_step = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)\n","\n","    \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","sess.run(init)\n","\n","saver.restore(sess, model_path)\n","\n","\n","train_loss_list = []\n","\n","mean_loss_list = []\n","    \n","timeperepoch = []\n","    \n","\n","\n","[Total_Data, row, col, depth] = np.shape(Data_Train)    \n","itr = 0\n","for i in range(num_epochs): \n","      \n","    index = np.random.permutation(Total_Data)\n","       \n","    Data_Train = Data_Train[index,:,:,:]\n","    \n","    LabelTrain_Targets = LabelTrain_Targets[index,:,:]\n","    LabelTrain_Outputs = LabelTrain_Outputs[index,:,:]\n","        \n","    for k in range(Total_Data//BatchSize):\n","        start = time.time()\n","            \n","        batch_input = Data_Train[k*BatchSize:(k+1)*BatchSize,:,:,:]\n","        batch_outputs = LabelTrain_Outputs[k*BatchSize:(k+1)*BatchSize,:,:]\n","      \n","        batch_targets = LabelTrain_Targets[k*BatchSize:(k+1)*BatchSize,:,:]\n","        train_feed_dict = {Inputs:batch_input, outputs:batch_outputs, targets:batch_targets, seq_length: seq_length_batch}\n","       \n","            \n","        _,train_loss,logits_Out = sess.run([training_step, loss,logits], train_feed_dict)\n","        \n","        \n","\n","               \n","        \n","        acc= sess.run([accuracy], feed_dict={logits: logits_Out,targets:batch_targets})\n","                                                       \n","                    \n","        \n","        itr = itr + 1\n","            \n","        train_loss_list.append(train_loss)\n","        end = time.time()\n","        timeperepoch.append(end-start)\n","      \n","        print('### L1 loss TRAINING ###')\n","        print ('Epoch{:3} : Step:{:3} : Train loss is :->{:>6.3f} Epoch duration :{:>6.3f}s'.format(i,k,train_loss,end- start)+ '\\n')\n","        print(\" accuracy_train\"+str(acc))\n","        print('### fin L1 loss TRAINING ###')\n","    saver = tf.train.Saver()    \n","    \n","      \n","        \n","        ################################################Fin Train#####################################\n","  \n","save_path = saver.save(sess, model_path)\n","print(\"Model saved in file: %s\" % save_path)\n","end_after_epoch=time.time() \n","\n"],"execution_count":null,"outputs":[]}]}